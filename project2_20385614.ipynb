{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readtxt(filename):\n",
    "    f = open(filename,'r')\n",
    "    txtlines=f.readlines()\n",
    "\n",
    "    images=[]\n",
    "    labels=[]\n",
    "    for line in txtlines:\n",
    "        filename=line[0:-3]\n",
    "        label=line[-2]\n",
    "        im=Image.open(filename)\n",
    "        im=im.resize((32, 32), Image.BILINEAR)\n",
    "        im=np.asarray(im)\n",
    "        images.append(im)\n",
    "        labels.append(int(label))\n",
    "    images=np.array(images)\n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readtst(filename):\n",
    "    f = open(filename,'r')\n",
    "    txtlines=f.readlines()\n",
    "\n",
    "    images=[]\n",
    "    for line in txtlines:\n",
    "        filename=line[0:-1]\n",
    "        im=Image.open(filename)\n",
    "        im=im.resize((32, 32), Image.BILINEAR)\n",
    "        im=np.asarray(im)\n",
    "        images.append(im)\n",
    "    images=np.array(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image,train_label=readtxt('train.txt')\n",
    "val_image,val_label=readtxt('val.txt')\n",
    "test_image=readtst('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train_image.astype('float32')\n",
    "x_val = val_image.astype('float32')\n",
    "x_test = test_image.astype('float32')\n",
    "x_train /= 255\n",
    "x_val /= 255\n",
    "x_test /=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (2569, 32, 32, 3)\n",
      "2569 train samples\n",
      "550 val samples\n",
      "551 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_val.shape[0], 'val samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 80\n",
    "num_classes = 5\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(train_label, num_classes)\n",
    "y_val = keras.utils.to_categorical(val_label, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, kernel_size=(5, 5),activation='relu',input_shape=(32,32,3),padding='same',strides=(1,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same',strides=(1,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',padding='same',strides=(1,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same',strides=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2569 samples, validate on 550 samples\n",
      "Epoch 1/50\n",
      "2569/2569 [==============================] - 12s 5ms/step - loss: 1.5620 - acc: 0.2787 - val_loss: 1.8074 - val_acc: 0.1891\n",
      "Epoch 2/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 1.4474 - acc: 0.3620 - val_loss: 1.4111 - val_acc: 0.3582\n",
      "Epoch 3/50\n",
      "2569/2569 [==============================] - 12s 5ms/step - loss: 1.3515 - acc: 0.4251 - val_loss: 1.2241 - val_acc: 0.4800\n",
      "Epoch 4/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 1.2589 - acc: 0.4585 - val_loss: 1.2523 - val_acc: 0.5109\n",
      "Epoch 5/50\n",
      "2569/2569 [==============================] - 12s 5ms/step - loss: 1.2014 - acc: 0.4951 - val_loss: 1.2656 - val_acc: 0.4255\n",
      "Epoch 6/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 1.1671 - acc: 0.5072 - val_loss: 1.5465 - val_acc: 0.3236\n",
      "Epoch 7/50\n",
      "2569/2569 [==============================] - 12s 5ms/step - loss: 1.1646 - acc: 0.5099 - val_loss: 1.3151 - val_acc: 0.4618\n",
      "Epoch 8/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 1.1309 - acc: 0.5418 - val_loss: 1.2280 - val_acc: 0.4818\n",
      "Epoch 9/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 1.1182 - acc: 0.5601 - val_loss: 1.1818 - val_acc: 0.5273\n",
      "Epoch 10/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 1.0819 - acc: 0.5675 - val_loss: 1.2529 - val_acc: 0.4927\n",
      "Epoch 11/50\n",
      "2569/2569 [==============================] - 12s 5ms/step - loss: 1.0729 - acc: 0.5707 - val_loss: 1.1409 - val_acc: 0.5491\n",
      "Epoch 12/50\n",
      "2569/2569 [==============================] - 12s 5ms/step - loss: 1.0292 - acc: 0.5952 - val_loss: 1.3594 - val_acc: 0.5127\n",
      "Epoch 13/50\n",
      "2569/2569 [==============================] - 12s 5ms/step - loss: 1.0279 - acc: 0.5975 - val_loss: 1.1778 - val_acc: 0.4818\n",
      "Epoch 14/50\n",
      "2569/2569 [==============================] - 12s 5ms/step - loss: 1.0019 - acc: 0.6069 - val_loss: 1.2558 - val_acc: 0.4455\n",
      "Epoch 15/50\n",
      "2569/2569 [==============================] - 12s 4ms/step - loss: 0.9872 - acc: 0.6115 - val_loss: 1.0423 - val_acc: 0.5873\n",
      "Epoch 16/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.9964 - acc: 0.6080 - val_loss: 1.0697 - val_acc: 0.5800\n",
      "Epoch 17/50\n",
      "2569/2569 [==============================] - 12s 5ms/step - loss: 0.9687 - acc: 0.6072 - val_loss: 0.9923 - val_acc: 0.6182\n",
      "Epoch 18/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.9519 - acc: 0.6279 - val_loss: 1.1352 - val_acc: 0.5600\n",
      "Epoch 19/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.9495 - acc: 0.6255 - val_loss: 1.0592 - val_acc: 0.5964\n",
      "Epoch 20/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.9387 - acc: 0.6263 - val_loss: 0.9899 - val_acc: 0.6127\n",
      "Epoch 21/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.9248 - acc: 0.6318 - val_loss: 1.0259 - val_acc: 0.5873\n",
      "Epoch 22/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.9133 - acc: 0.6520 - val_loss: 1.0087 - val_acc: 0.6036\n",
      "Epoch 23/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.8849 - acc: 0.6520 - val_loss: 1.0159 - val_acc: 0.5836\n",
      "Epoch 24/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.9031 - acc: 0.6497 - val_loss: 1.1459 - val_acc: 0.5545\n",
      "Epoch 25/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.9029 - acc: 0.6563 - val_loss: 1.0025 - val_acc: 0.6127\n",
      "Epoch 26/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.8627 - acc: 0.6769 - val_loss: 0.9677 - val_acc: 0.6091\n",
      "Epoch 27/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.8492 - acc: 0.6722 - val_loss: 1.0924 - val_acc: 0.5764\n",
      "Epoch 28/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.8472 - acc: 0.6695 - val_loss: 1.1525 - val_acc: 0.5709\n",
      "Epoch 29/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.8565 - acc: 0.6637 - val_loss: 1.0399 - val_acc: 0.5982\n",
      "Epoch 30/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.8159 - acc: 0.6948 - val_loss: 0.9424 - val_acc: 0.6564\n",
      "Epoch 31/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.8158 - acc: 0.6902 - val_loss: 0.9988 - val_acc: 0.6073\n",
      "Epoch 32/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.8201 - acc: 0.6820 - val_loss: 0.9487 - val_acc: 0.6182\n",
      "Epoch 33/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.7946 - acc: 0.6991 - val_loss: 0.9152 - val_acc: 0.6327\n",
      "Epoch 34/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.7796 - acc: 0.7030 - val_loss: 1.0196 - val_acc: 0.6436\n",
      "Epoch 35/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.7808 - acc: 0.6917 - val_loss: 1.0294 - val_acc: 0.6436\n",
      "Epoch 36/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.7878 - acc: 0.7069 - val_loss: 0.9296 - val_acc: 0.6418\n",
      "Epoch 37/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.7571 - acc: 0.7061 - val_loss: 0.9997 - val_acc: 0.6145\n",
      "Epoch 38/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.7569 - acc: 0.7092 - val_loss: 0.9113 - val_acc: 0.6582\n",
      "Epoch 39/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.7401 - acc: 0.7151 - val_loss: 0.8938 - val_acc: 0.6655\n",
      "Epoch 40/50\n",
      "2569/2569 [==============================] - 10s 4ms/step - loss: 0.7171 - acc: 0.7287 - val_loss: 0.9725 - val_acc: 0.6473\n",
      "Epoch 41/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.7196 - acc: 0.7236 - val_loss: 1.1201 - val_acc: 0.5836\n",
      "Epoch 42/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.7085 - acc: 0.7306 - val_loss: 0.9489 - val_acc: 0.6091\n",
      "Epoch 43/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.6996 - acc: 0.7361 - val_loss: 1.2249 - val_acc: 0.5727\n",
      "Epoch 44/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.6788 - acc: 0.7439 - val_loss: 0.9509 - val_acc: 0.6218\n",
      "Epoch 45/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.6798 - acc: 0.7446 - val_loss: 1.2901 - val_acc: 0.5309\n",
      "Epoch 46/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.6817 - acc: 0.7470 - val_loss: 0.8981 - val_acc: 0.6673\n",
      "Epoch 47/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.6504 - acc: 0.7602 - val_loss: 0.8644 - val_acc: 0.6709\n",
      "Epoch 48/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.6493 - acc: 0.7575 - val_loss: 0.9170 - val_acc: 0.6600\n",
      "Epoch 49/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.6533 - acc: 0.7575 - val_loss: 1.0386 - val_acc: 0.6036\n",
      "Epoch 50/50\n",
      "2569/2569 [==============================] - 11s 4ms/step - loss: 0.6525 - acc: 0.7544 - val_loss: 0.8933 - val_acc: 0.6655\n",
      "Test loss: 0.893288650513\n",
      "Test accuracy: 0.665454545671\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_val, y_val))\n",
    "score = model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551/551 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, 4, 1, 4, 1, 1, 3, 4, 4, 3, 1, 0, 2, 3, 4, 0, 1, 1, 4, 1, 4,\n",
       "       0, 4, 3, 1, 0, 4, 3, 4, 4, 1, 1, 0, 3, 4, 2, 1, 3, 0, 3, 1, 3, 4, 4,\n",
       "       2, 1, 1, 0, 0, 4, 4, 4, 4, 1, 1, 4, 1, 4, 4, 3, 1, 1, 3, 3, 4, 2, 2,\n",
       "       4, 2, 1, 4, 2, 3, 1, 4, 4, 3, 4, 3, 0, 1, 3, 1, 0, 4, 4, 1, 3, 4, 4,\n",
       "       3, 3, 0, 1, 1, 3, 1, 3, 4, 1, 4, 3, 2, 0, 1, 1, 2, 2, 0, 1, 4, 4, 1,\n",
       "       3, 4, 1, 4, 3, 1, 4, 3, 1, 1, 1, 3, 3, 2, 4, 0, 1, 4, 4, 4, 4, 2, 3,\n",
       "       3, 4, 1, 2, 4, 4, 1, 1, 1, 0, 2, 3, 3, 2, 1, 4, 4, 1, 1, 0, 1, 1, 4,\n",
       "       4, 3, 3, 1, 3, 4, 4, 4, 4, 4, 1, 1, 3, 2, 4, 0, 1, 2, 3, 4, 2, 1, 4,\n",
       "       2, 4, 3, 0, 0, 3, 4, 1, 4, 2, 0, 1, 4, 2, 1, 4, 3, 1, 2, 1, 4, 1, 4,\n",
       "       0, 3, 0, 4, 0, 4, 4, 4, 1, 3, 4, 3, 4, 1, 1, 1, 4, 3, 1, 0, 2, 1, 1,\n",
       "       1, 1, 4, 2, 3, 1, 4, 0, 2, 2, 3, 4, 0, 4, 0, 4, 0, 3, 3, 4, 4, 3, 0,\n",
       "       1, 0, 4, 3, 3, 0, 2, 4, 2, 4, 3, 4, 1, 4, 1, 4, 3, 1, 3, 1, 3, 4, 4,\n",
       "       0, 4, 0, 3, 3, 2, 1, 0, 1, 4, 4, 3, 3, 1, 4, 0, 3, 1, 0, 1, 1, 0, 1,\n",
       "       2, 1, 4, 1, 2, 4, 1, 4, 2, 1, 4, 4, 0, 0, 1, 2, 4, 3, 1, 1, 2, 0, 0,\n",
       "       3, 2, 4, 4, 2, 0, 2, 2, 1, 2, 3, 1, 3, 4, 4, 0, 4, 4, 4, 0, 1, 3, 0,\n",
       "       1, 4, 4, 3, 0, 1, 1, 1, 1, 4, 3, 3, 4, 0, 1, 1, 4, 1, 3, 0, 2, 4, 0,\n",
       "       4, 3, 4, 1, 3, 4, 1, 4, 3, 4, 4, 1, 4, 2, 2, 4, 3, 3, 4, 1, 4, 4, 4,\n",
       "       2, 1, 0, 1, 1, 1, 2, 4, 3, 3, 1, 4, 4, 4, 1, 1, 1, 1, 3, 3, 1, 4, 1,\n",
       "       3, 0, 4, 4, 2, 3, 1, 0, 2, 4, 4, 4, 2, 1, 4, 4, 3, 4, 3, 1, 1, 4, 3,\n",
       "       4, 1, 4, 4, 0, 1, 3, 2, 3, 3, 1, 1, 3, 0, 4, 4, 4, 3, 3, 3, 3, 4, 4,\n",
       "       4, 0, 0, 1, 4, 1, 4, 2, 4, 1, 0, 2, 0, 3, 3, 4, 0, 4, 0, 3, 3, 1, 4,\n",
       "       4, 1, 3, 1, 0, 2, 1, 3, 2, 3, 1, 3, 2, 1, 0, 0, 1, 3, 2, 2, 0, 3, 0,\n",
       "       2, 0, 1, 0, 2, 2, 1, 3, 3, 3, 3, 3, 1, 4, 4, 4, 4, 4, 1, 1, 2, 1, 4,\n",
       "       4, 4, 4, 2, 1, 1, 4, 1, 4, 1, 4, 3, 4, 0, 3, 1, 1, 0, 1, 3, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_label=np.ndarray.tolist(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('project2_20385614.txt','w')\n",
    "for x in test_label:\n",
    "    f.write(\"%s\\n\" % x) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
